/*
* Copyright (c) 2016, 2017 Pedro Falcato
* This file is part of Onyx, and is released under the terms of the MIT License
* check LICENSE at the root directory for more information
*/
#include <signal.h>
#include <stdio.h>
#include <errno.h>

#include <onyx/cpu.h>
#include <onyx/vm.h>
#include <onyx/signal.h>
#include <onyx/panic.h>
#include <onyx/process.h>
#include <onyx/task_switching.h>
#include <onyx/wait_queue.h>
#include <onyx/clock.h>
#include <onyx/pid.h>
#include <onyx/memory.hpp>

void signal_default_term(int signum)
{
	process_exit_from_signal(signum);
}

void signal_default_core(int signum)
{
	/* TODO: Generate a core dump */
	signal_default_term(signum);
}

void signal_default_ignore(int signum)
{
	(void) signum;
}

atomic<unsigned int> stopped = 0;

void signal_do_stop(int signum)
{
	struct process *current = get_current_process();
	auto current_thread = get_current_thread();

	if(!(current_thread->sinfo.flags & THREAD_SIGNAL_STOPPING))
	{
		/* For every thread in the process, tell it to stop */
		process_for_every_thread(current, [&](thread *t) -> bool
		{
			if(t == current_thread)
				return true;

			scoped_lock g{t->sinfo.lock};
			t->sinfo.flags |= THREAD_SIGNAL_STOPPING;
			t->sinfo.signal_pending = true;

			if(t->status == THREAD_INTERRUPTIBLE)
				thread_wake_up(t);

			return true;
		});

		current_thread->sinfo.flags |= THREAD_SIGNAL_STOPPING;
		
		current->signal_group_flags |= SIGNAL_GROUP_STOPPED;
		current->signal_group_flags &= ~SIGNAL_GROUP_CONT;

		/* TODO: The last thread to stop should do this, not the first. */

		notify_process_stop_cont(current, signum);
	}

	set_current_state(THREAD_STOPPED);

	stopped++;
	spin_unlock(&current_thread->sinfo.lock);
	spin_unlock(&current->signal_lock);

	sched_yield();
	stopped--;

	spin_lock(&current->signal_lock);
	spin_lock(&current_thread->sinfo.lock);
}

/* This table only handles non-realtime signals (so, from signo 1 to 31, inclusive) */
sighandler_t dfl_signal_handlers[] = {
	signal_default_term,
	/*[SIGHUP] = */ signal_default_term,
	/*[SIGINT] = */signal_default_term,
	/*[SIGQUIT] = */ signal_default_core,
	/*[SIGILL] = */ signal_default_core,
	/*[SIGTRAP] = */signal_default_core,
	/*[SIGABRT] =*/ signal_default_core,
	/*[SIGBUS] =*/ signal_default_core,
	/*[SIGFPE] =*/ signal_default_core,
	/*[SIGKILL] =*/ signal_default_term,
	/*[SIGUSR1] =*/ signal_default_term,
	/*[SIGSEGV] =*/ signal_default_core,
	/*[SIGUSR2] =*/ signal_default_term,
	/*[SIGPIPE] =*/ signal_default_term,
	/*[SIGALRM] =*/ signal_default_term,
	/*[SIGTERM] =*/ signal_default_term,
	/*[SIGSTKFLT] =*/ signal_default_term,
	/*[SIGCHLD] =*/ signal_default_ignore,
	/*[SIGCONT] =*/ signal_default_ignore,
	/*[SIGSTOP] =*/ signal_do_stop,
	/*[SIGTSTP] =*/ signal_do_stop,
	/*[SIGTTIN] =*/ signal_do_stop,
	/*[SIGTTOU] =*/ signal_do_stop,
	/*[SIGURG] =*/ signal_default_ignore,
	/*[SIGXCPU] =*/ signal_default_core,
	/*[SIGXFSZ] =*/ signal_default_core,
	/*[SIGVTALRM] =*/ signal_default_term,
	/*[SIGPROF] =*/ signal_default_term,
	/*[SIGWINCH] =*/ signal_default_ignore,
	/*[SIGIO] =*/ signal_default_ignore,
	/*[SIGPWR] =*/ signal_default_ignore,
	/*[SIGSYS] =*/ signal_default_core
};

#define SST_SIZE (_NSIG/8/sizeof(long))
void signotset(sigset_t *set)
{
	for(size_t i = 0; i < SST_SIZE; i++)
		set->__bits[i] = ~set->__bits[i];
}

bool signal_is_unblockable(int signum)
{
	switch(signum)
	{
		case SIGSTOP:
		case SIGKILL:
			return true;
	}

	return false;
}

void do_default_signal(int signum, struct sigpending *pend)
{
	
	/* For realtime signals (which we don't include in the dfl_signal_handlers), the default action
	 * is to terminate the process.
	 */
	bool is_term = signal_is_realtime(signum)
			|| dfl_signal_handlers[signum] == signal_default_term
			|| dfl_signal_handlers[signum] == signal_default_core;
	
	if(!is_term) [[likely]]
	{
		dfl_signal_handlers[signum](signum);
		return;
	}

	dfl_signal_handlers[signum](signum);

	__builtin_unreachable();
}

int signal_find(struct thread *thread)
{
	sigset_t *set = &thread->sinfo.pending_set;
	sigset_t *blocked_set = &thread->sinfo.sigmask;

	for(int i = 0; i < NSIG; i++)
	{
		if(sigismember(set, i) && !sigismember(blocked_set, i))
		{
			assert(i != 0);
			return i;
		}
	}

	return -1;
}

bool signal_is_empty(struct thread *thread)
{
	sigset_t *set = &thread->sinfo.pending_set;
	sigset_t *blocked_set = &thread->sinfo.sigmask;
	for(int i = 0; i < NSIG; i++)
	{
		if(sigismember(set, i) && !sigismember(blocked_set, i))
			return false;
	}

	return true;
}

#define SIGNAL_QUERY_POP			(1 << 0)

struct sigpending *signal_query_pending(int signum, unsigned int flags, struct signal_info *info)
{
	list_for_every(&info->pending_head)
	{
		struct sigpending *pend = container_of(l, struct sigpending, list_node);
		
		if(pend->signum == signum)
		{
			/* Found one! */
			if(flags & SIGNAL_QUERY_POP)
				list_remove(&pend->list_node);
			return pend;
		}
	}

	return NULL;
}

bool deliver_signal(int signum, struct sigpending *pending, struct registers *regs);

/* Returns negative if deliver_signal shouldn't execute the rest of the code, and should return immediately */
int force_sigsegv(struct sigpending *pending, struct registers *regs)
{
	int signum = pending->signum;

	pending->info->si_code = SI_KERNEL;
	pending->info->si_signo = SIGSEGV;
	pending->info->si_addr = NULL;

	/* If we were trying to deliver SEGV; just do the default signal */
	if(signum == SIGSEGV)
	{
		do_default_signal(signum, pending);
	}
	else
	{
		/* Else, try to deliver a SIGSEGV */
		deliver_signal(SIGSEGV, pending, regs);
		/* Explicitly return here in order not to execute the rest of the code */
		return -1;
	}

	return 0;
}

void signal_unqueue(int signum, struct thread *thread)
{
	bool is_realtime_signal = signum >= KERNEL_SIGRTMIN;
	bool should_delete = true;

	if(is_realtime_signal)
	{
		/* Search the query'ed backlog to see if there are other
		 * realtime signals(of the same signum, of course) queued.
		 */

		should_delete = signal_query_pending(signum, 0, &thread->sinfo) == NULL;
	}

	if(should_delete)
	{
		sigdelset(&thread->sinfo.pending_set, signum);
	}

	thread->sinfo.__update_pending();
}

bool signal_uncatcheable(int signum)
{
	return signal_is_stopping(signum) || signum == SIGKILL;
}

bool deliver_signal(int signum, struct sigpending *pending, struct registers *regs)
{
	struct thread *thread = get_current_thread();
	struct process *process = thread->owner;

	struct k_sigaction *k_sigaction = &process->sigtable[signum];
	void (*handler)(int) = k_sigaction->sa_handler;
	bool defer_user = false;

	/* TODO: Handle SA_RESTART */
	/* TODO: Handle SA_NOCLDWAIT */
	/* TODO: Handle SA_NOCLDSTOP */

	if(handler != SIG_DFL && !signal_uncatcheable(signum))
	{
		defer_user = true;
		if(signal_setup_context(pending, k_sigaction, regs) < 0)
		{
			if(force_sigsegv(pending, regs) < 0)
				return true;
		}
	}
	else
	{
		do_default_signal(signum, pending);
	}

	if(k_sigaction->sa_flags & SA_RESETHAND)
	{
		/* If so, we need to reset the handler to SIG_DFL and clear SA_SIGINFO */
		k_sigaction->sa_handler = SIG_DFL;
		k_sigaction->sa_flags &= ~SA_SIGINFO;
	}
	
	sigset_t new_blocked;
	memcpy(&new_blocked, &k_sigaction->sa_mask, sizeof(new_blocked));

	if(!(k_sigaction->sa_flags & SA_NODEFER))
	{
		/* POSIX specifies that the signal needs to be blocked while being handled */
		sigaddset(&new_blocked, signum);
	}

	// Re-lock it
	scoped_lock g{thread->sinfo.lock};

	thread->sinfo.__add_blocked(&new_blocked);

	signal_unqueue(signum, thread);

	return defer_user;
}

unsigned long sched_get_preempt_counter(void);

void handle_signal(struct registers *regs)
{
	/* We can't do signals while in kernel space */
	if(in_kernel_space_regs(regs))
	{
		return;
	}

	context_tracking_enter_kernel();

	if(irq_is_disabled())
		irq_enable();

	struct thread *thread = get_current_thread();

	thread->sinfo.times_interrupted++;

	struct process *process = thread->owner;

	scoped_lock g{process->signal_lock};

	scoped_lock g2{thread->sinfo.lock};

	/* This infinite loop should speed things up by letting us handle things
	 * like getting SIGSTOP'd and then handling the SIGCONT in the same interruption/kernel exit.
	 */
	while(true)
	{
		if(thread->sinfo.flags & THREAD_SIGNAL_EXITING)
		{
			g2.unlock();
			g.unlock();
			thread_exit();
		}

		if(thread->sinfo.flags & THREAD_SIGNAL_STOPPING)
		{
			signal_do_stop(0);
			continue;
		}

		/* Find an available signal */
		int signum = signal_find(thread);

		/* Oh no, no more signals :(( */
		if(signum < 0)
			break;

		auto pending = signal_query_pending(signum, SIGNAL_QUERY_POP, &thread->sinfo);

		assert(pending != NULL);

		// We need to unlock and relock the process and thread signal locks due to copy_to/from_user, which may sleep
		g2.unlock();
		g.unlock();

		bool defer = deliver_signal(signum, pending, regs);
		
		g.lock();
		g2.lock();
		
		if (defer)
			break;
	}

	context_tracking_exit_kernel();

}

int kernel_raise_signal(int sig, struct process *process, unsigned int flags, siginfo_t *info)
{
	struct thread *t = NULL;

	spin_lock(&process->thread_list_lock);

	list_for_every(&process->thread_list)
	{
		struct thread *thr = container_of(l, struct thread, thread_list_head);

		if(!sigismember(&thr->sinfo.sigmask, sig))
		{
			t = thr;
			break;
		}
	}

	if(t == NULL)
	{
		/* If the signal is masked everywhere, just pick the first thread... */
		t = container_of(list_first_element(&process->thread_list), struct thread,
			thread_list_head);
	}

	assert(t != NULL);

	thread_get(t);

	spin_unlock(&process->thread_list_lock);
	
	int st = kernel_tkill(sig, t, flags, info);

	thread_put(t);

	return st;
}

void do_signal_force_unblock(int signal, struct thread *thread)
{
	/* Do it like Linux, and restore the handler to SIG_DFL,
	 * and unmask the thread
	 */

	struct process *process = thread->owner;

	process->sigtable[signal].sa_handler = SIG_DFL;
	sigdelset(&thread->sinfo.sigmask, signal);
}

int may_kill(int signum, struct process *target, siginfo_t *info)
{
	bool is_kernel = !info || info->si_code > 0;
	int st = 0;

	if(is_kernel)
		return 0;

	struct creds *c = creds_get();
	struct creds *other = NULL;
	if(c->euid == 0)
		goto out;
	
	other = __creds_get(target);
	if(c->euid == other->ruid || c->euid == other->suid ||
	   c->ruid == other->ruid || c->ruid == other->suid)
		st = 0;
	else
		st = -1;

out:
	if(other)	creds_put(other);
	creds_put(c);
	return st;
}

static bool is_default_ignored(int signal)
{
	return dfl_signal_handlers[signal] == signal_default_ignore;
}

static bool is_signal_ignored(struct process *process, int signal)
{
	return process->sigtable[signal].sa_handler == SIG_IGN ||
	       (process->sigtable[signal].sa_handler == SIG_DFL && is_default_ignored(signal));
}

bool signal_may_wake(int signum)
{
	return signum == SIGCONT || signum == SIGKILL;
}

atomic<unsigned int> woke = 0;

void signal_do_special_behaviour(int signal, struct thread *thread)
{
	auto proc = thread->owner;

	if(signal == SIGCONT)
	{
		/* If any stop signals are pending, unqueue them */
		signal_unqueue(SIGSTOP, thread);
		signal_unqueue(SIGTSTP, thread);
		signal_unqueue(SIGTTIN, thread);
		signal_unqueue(SIGTTOU, thread);

		process_for_every_thread(proc, [&](struct thread *t) -> bool
		{
			bool should_lock = thread != t;

			/* The locks should make this immune to races */
			if(should_lock) spin_lock(&t->sinfo.lock);

			if(t->sinfo.flags & THREAD_SIGNAL_STOPPING)
			{
				thread_wake_up(t);
				woke++;
			}

			t->sinfo.flags &= ~THREAD_SIGNAL_STOPPING;

			t->sinfo.__update_pending();

			if(should_lock) spin_unlock(&t->sinfo.lock);

			return true;
		});

		bool was_stopped = proc->signal_group_flags & SIGNAL_GROUP_STOPPED;

		proc->signal_group_flags &= ~SIGNAL_GROUP_STOPPED;
	
		if(was_stopped)
		{
			proc->signal_group_flags |= SIGNAL_GROUP_CONT;
			notify_process_stop_cont(proc, SIGCONT);
		}
	}
	else if(signal_is_stopping(signal) && proc->sigtable[signal].sa_handler == SIG_DFL)
	{
		signal_unqueue(SIGCONT, thread);
	}
	else if(signal == SIGKILL)
	{
		signal_unqueue(SIGSTOP, thread);
		signal_unqueue(SIGTSTP, thread);
		signal_unqueue(SIGTTIN, thread);
		signal_unqueue(SIGTTOU, thread);

		process_for_every_thread(proc, [&](struct thread *t) -> bool
		{
			bool should_lock = thread != t;

			/* The locks should make this immune to races */
			if(should_lock) spin_lock(&t->sinfo.lock);

			if(t->sinfo.flags & THREAD_SIGNAL_STOPPING)
			{
				thread_wake_up(t);
				woke++;
			}

			t->sinfo.flags &= ~THREAD_SIGNAL_STOPPING;
			t->sinfo.__update_pending();

			if(should_lock) spin_unlock(&t->sinfo.lock);

			return true;
		});
	}
}

int kernel_tkill(int signal, struct thread *thread, unsigned int flags, siginfo_t *info)
{
	struct process *process = thread->owner;
	unique_ptr<struct sigpending> pending;
	siginfo_t *copy_siginfo = nullptr;

	if(may_kill(signal, process, info) < 0)
		return -EPERM;

	scoped_lock g{process->signal_lock};
	scoped_lock g2{thread->sinfo.lock};

	signal_do_special_behaviour(signal, thread);

	/* Don't bother to set it as pending if sig == SIG_IGN or it's set to the default
	 * and the default is to ignore.
	 */
	bool is_signal_ign = is_signal_ignored(process, signal) && !(signal_is_unblockable(signal));

	bool is_masked = sigismember(&thread->sinfo.sigmask, signal);

	bool signal_delivery_blocked = is_signal_ign || is_masked;

	if(flags & SIGNAL_FORCE && signal_delivery_blocked)
	{
		/* If the signal delivery is being forced for some reason
		 * (usually, it's because of a hardware exception), we'll need
		 * to unblock it forcefully.
		 */
		do_signal_force_unblock(signal, thread);
	}
	else if(is_signal_ign)
	{
		return 0;
	}

	bool standard_signal = signal < KERNEL_SIGRTMIN;

	if(standard_signal && sigismember(&thread->sinfo.pending_set, signal))
	{
		/* Already signaled, return success */
		goto success;
	}

	pending = make_unique<struct sigpending>();
	if(!pending)
	{
		goto failure_oom;
	}

	copy_siginfo = new siginfo_t;
	if(!copy_siginfo)
	{
		goto failure_oom;
	}

	if(info)
	{
		memcpy(copy_siginfo, info, sizeof(siginfo_t));
	}
	else
	{
		memset(copy_siginfo, 0, sizeof(siginfo_t));
		copy_siginfo->si_code = SI_KERNEL;
	}

	copy_siginfo->si_signo = signal;

	pending->info = copy_siginfo;
	pending->signum = signal;

	list_add(&pending->list_node, &thread->sinfo.pending_head);

	pending.release();

	sigaddset(&thread->sinfo.pending_set, signal);

	if(!sigismember(&thread->sinfo.sigmask, signal))
	{
		thread->sinfo.signal_pending = true;
		/* We're only waking the thread up for two reasons: It's either in an interruptible sleep
		 * OR it's stopped and we're SIGCONT'ing it */
		if(thread->status == THREAD_INTERRUPTIBLE || (thread->status == THREAD_STOPPED
		   && signal_may_wake(signal)))
			thread_wake_up(thread);
	}

success:
	return 0;

failure_oom:

	if(flags & SIGNAL_FORCE)
	{
		/* I don't think there's another way to do this, for now */
		/* Our kernel's OOM behavior and mechanisms are iffy *at best* */
		panic("SIGNAL_FORCE couldn't be done");
	}

	return -ENOMEM;
}

bool signal_is_masked(struct thread *thread, int sig)
{
	sigset_t *set = &thread->sinfo.sigmask;
	return (bool) sigismember(set, sig);
}

bool is_valid_signal(int sig)
{
	return sig > 0 && sig < NSIG;
}

struct send_all_info
{
	int signals_sent;
	siginfo_t *info;
	int signum;

	constexpr send_all_info(siginfo_t *i, int sig) : signals_sent{}, info{i}, signum{sig} {}
};

bool pid_is_system_process(pid_t pid)
{
	return pid == 1;
}

int signal_send_all(int signal, int flags, siginfo_t *info)
{
	send_all_info i{info, signal};

	for_every_process([](process *p, void *ctx) -> bool
	{
		/* Do not allow signalling pid 1 and ourselves. */
		if(pid_is_system_process(p->get_pid()) || p == get_current_process())
			return true;
		
		send_all_info *c = (send_all_info *) ctx;
	
		if(may_kill(c->signum, p, c->info) < 0)
			return true;

		if(kernel_raise_signal(c->signum, p, 0, c->info) < 0)
			return true;
		
		c->signals_sent++;
		return true;	
	}, &i);

	return i.signals_sent != 0 ? 0 : -EPERM;
}

pid::auto_pid process_get_pgrp(process *p)
{
	scoped_lock g{p->pgrp_lock};
	
	auto pg = p->process_group;

	pg->ref();

	return pg;
}

int signal_kill_pg(int sig, int flags, siginfo_t *info, pid_t pid)
{
	bool own = pid == 0;

	pid::auto_pid pgrp_res = own ? process_get_pgrp(get_current_process()) : pid::lookup(-pid);

	if(!pgrp_res)
		return -ESRCH;
	
	return pgrp_res->kill_pgrp(sig, flags, info);
}

int sys_kill(pid_t pid, int sig)
{
	int st = 0;
	struct process *p = NULL;
	struct creds *c = NULL;
	siginfo_t info = {};

	if(pid > 0)
	{
		p = get_process_from_pid(pid);
		if(!p)
			return -ESRCH;	
	}

	if(sig == 0)
	{
		goto out;
	}
	
	if(!is_valid_signal(sig))
	{
		st = -EINVAL;
		goto out;
	}

	c = creds_get();

	info.si_signo = sig;
	info.si_code = SI_USER;
	info.si_uid = c->euid;
	info.si_pid = get_current_process()->get_pid();

	creds_put(c);

	if(pid <= 0)
	{
		if(pid == -1)
		{
			st = signal_send_all(sig, 0, &info);
		}
		else if(pid < -1 || pid == 0)
		{
			st = signal_kill_pg(sig, 0, &info, pid);
		}
	}
	else
		st = kernel_raise_signal(sig, p, 0, &info);

out:
	if(p) process_put(p);
	return st;
}

int sys_sigaction(int signum, const struct k_sigaction *act, struct k_sigaction *oldact)
{
	int st = 0;
	if(!is_valid_signal(signum))
		return -EINVAL;

	/* If both pointers are NULL, just return 0 (We can't do anything) */
	if(!oldact && !act)
		return 0;

	struct process *proc = get_current_process();

	/* Lock the signal table */
	scoped_lock g{proc->signal_lock};

	/* If old_act, save the old action */
	if(oldact)
	{
		if(copy_to_user(oldact, &proc->sigtable[signum], sizeof(struct k_sigaction)) < 0)
			return -EFAULT;
	}

	/* If act, set the new action */
	if(act)
	{
		struct k_sigaction sa;

		if(copy_from_user(&sa, act, sizeof(struct k_sigaction)) < 0)
			return -EFAULT;

		if(sa.sa_handler == SIG_ERR)
			return -EINVAL;

		/* Check if it's actually possible to set a handler to this signal */
		switch(signum)
		{
			/* If not, return EINVAL */
			case SIGKILL:
			case SIGSTOP:
				return -EINVAL;
		}

		memcpy(&proc->sigtable[signum], &sa, sizeof(sa));
	}

	return st;
}

int sys_sigprocmask(int how, const sigset_t *set, sigset_t *oldset, size_t sigset_size)
{
	if(sigset_size != sizeof(sigset_t))
		return -EINVAL;

	struct thread *current = get_current_thread();
	sigset_t koldset = current->sinfo.get_mask();

#if 0
	if(set) printk("sigprocmask %d %lx\n", how, set->__bits[0]);
#endif

	if(set)
	{
		sigset_t kset;
		if(copy_from_user(&kset, set, sizeof(sigset_t)) < 0)
			return -EFAULT;

		switch(how)
		{
			case SIG_BLOCK:
			{
				current->sinfo.add_blocked(&kset);
				break;
			}
			case SIG_UNBLOCK:
			{
				current->sinfo.unblock(kset);
				break;
			}
			case SIG_SETMASK:
			{
				current->sinfo.set_blocked(&kset);
				break;
			}
			default:
				return -EINVAL;
		}
	}

	if(oldset)
	{
		if(copy_to_user(oldset, &koldset, sizeof(sigset_t)) < 0)
			return -EFAULT;
	}

	return 0;
}

bool signal_is_pending(void)
{
	struct thread *t = get_current_thread();
	if(!t)
		return false;
#if 0
	if(t->sinfo.signal_pending == true)
		printk("Signal pending!\n");
#endif

	return t->sinfo.signal_pending;
}

int sys_sigsuspend(const sigset_t *uset)
{
	struct thread *current = get_current_thread();

	sigset_t set;
	if(copy_from_user(&set, uset, sizeof(sigset_t)) < 0)
		return -EFAULT;

	/* Ok, mask the signals in set */
	sigset_t old;
	/* First, save the old sigset */
	memcpy(&old, &current->sinfo.sigmask, sizeof(sigset_t));
	/* Now, set the signal mask */
	current->sinfo.set_blocked(&set);

	/* Now, wait for a signal */	
	struct wait_queue wq;
	init_wait_queue_head(&wq);

	wait_for_event_interruptible(&wq, false);

	current->sinfo.set_blocked(&old);

	return -EINTR;
}

int sys_pause(void)
{
	struct wait_queue wq;
	init_wait_queue_head(&wq);

	wait_for_event_interruptible(&wq, false);

	return -EINTR;
}

#define TGKILL_CHECK_PID			(1 << 0)
#define TGKILL_SIGQUEUE				(1 << 1)

int do_tgkill(int pid, int tid, int sig, unsigned int flags, siginfo_t *kinfo)
{
	int st = 0;
	siginfo_t info = {};

	if(tid < 0)
		return -EINVAL;

	struct thread *t = thread_get_from_tid(tid);
	if(!t)
	{
		return -ESRCH;
	}
	
	/* Can't send signals to kernel threads */
	if(t->flags & THREAD_KERNEL)
	{
		st = -EINVAL;
		goto out;
	}

	if(flags & TGKILL_CHECK_PID && t->owner->get_pid() != pid)
	{
		st = -ESRCH;
		goto out;
	}

	if(!is_valid_signal(sig))
	{
		st = -EINVAL;
		goto out;
	}
	
	if(!(flags & TGKILL_SIGQUEUE))
	{
		struct creds *c = creds_get();

		info.si_signo = sig;
		info.si_code = SI_TKILL;
		info.si_uid = c->euid;
		info.si_pid = get_current_process()->get_pid();

		creds_put(c);
	}
	else
	{
		memcpy(&info, kinfo, sizeof(info));
	}

	st = kernel_tkill(sig, t, 0, &info);

out:
	thread_put(t);

	return st;
}

int sys_tkill(int tid, int sig)
{
	return do_tgkill(-1, tid, sig, 0, NULL);
}

int sys_tgkill(int pid, int tid, int sig)
{
	return do_tgkill(pid, tid, sig, TGKILL_CHECK_PID, NULL);
}

int sanitize_rt_sigqueueinfo(siginfo_t *info, pid_t pid)
{
	struct process *current = get_current_process();

	if(current->get_pid() == pid)
		return 0;
	
	if(info->si_code >= 0)
		return -1;
	if(info->si_code == SI_TKILL)
		return -1;
	
	return 0;
}

int sys_rt_sigqueueinfo(pid_t pid, int sig, siginfo_t *uinfo)
{
	int st = 0;
	siginfo_t info;
	if(copy_from_user(&info, uinfo, sizeof(info)) < 0)
		return -EFAULT;

	if(sanitize_rt_sigqueueinfo(&info, pid) < 0)
		return -EPERM;

	struct process *p = get_process_from_pid(pid);
	if(!p)
		return -ESRCH;

	if(sig == 0)
	{
		goto out;
	}
	
	if(!is_valid_signal(sig))
	{
		st = -EINVAL;
		goto out;
	}

	st = kernel_raise_signal(sig, p, 0, &info);

out:
	process_put(p);
	return st;
}

int sys_rt_tgsigqueueinfo(pid_t pid, pid_t tid, int sig, siginfo_t *uinfo)
{
	siginfo_t info;
	if(copy_from_user(&info, uinfo, sizeof(info)) < 0)
		return -EFAULT;
	
	if(sanitize_rt_sigqueueinfo(&info, pid) < 0)
		return -EPERM;

	return do_tgkill(pid, tid, sig, TGKILL_CHECK_PID | TGKILL_SIGQUEUE, &info);
}

void signal_do_execve(struct process *proc)
{
	/* Clear the non-ignored signal disposition */
	for(int i = 0; i < NSIG; i++)
	{
		struct k_sigaction *sa = &proc->sigtable[i];
		if(sa->sa_handler != SIG_IGN)
			sa->sa_handler = NULL;
		
		sa->sa_flags = 0;
		memset(&sa->sa_mask, 0, sizeof(sa->sa_mask));
		sa->sa_restorer = NULL;
	}

	/* Clear the altstack */
	struct thread *t = get_current_thread();
	
	memset(&t->sinfo.altstack, 0, sizeof(t->sinfo.altstack));
	t->sinfo.altstack.ss_flags = SS_DISABLE;
}

#define CURRENT_SIGSETLEN				8

/* We need to separate this function since you can't have 2 wait_for_events */
/* TODO: Maybe fix it? */
long sigtimedwait_forever(struct wait_queue *wq)
{
	return wait_for_event_interruptible(wq, false);
}

int sys_rt_sigtimedwait(const sigset_t *set, siginfo_t *info, const struct timespec *utimeout, size_t sigsetlen)
{
	if(sigsetlen != CURRENT_SIGSETLEN)
		return -EINVAL;

	struct sigpending *pending = NULL;
	int st = 0;
	struct process *process = get_current_process();
	struct thread *thread = get_current_thread();

	struct wait_queue wq;
	init_wait_queue_head(&wq);

	struct timespec timeout = {};

	if(utimeout && copy_from_user(&timeout, utimeout, sizeof(timeout)) < 0)
		return -EFAULT;

	if(!timespec_valid(&timeout, false))
		return -EINVAL;
	
	sigset_t kset;

	if(copy_from_user(&kset, set, sizeof(kset)) < 0)
		return -EFAULT;

	/* TODO: The implementation is not quite correct */
#if 0
	sigset_t old;
	/* Save the old blocked set */
	memcpy(&old, &thread->sinfo.sigmask, sizeof(sigset_t));
	
	/* Silently ignore all attempts to wait for SIGKILL and SIGSTOP */
	sigdelset(&kset, SIGKILL);
	sigdelset(&kset, SIGSTOP);

	/* We invert the sigset in order to know what we need to block, and then
	 * we AND it with the old blocked set so we know what we're actually blocking.
	 */
	signotset(&kset);

	{

	scoped_lock g{thread->sinfo.lock};
	sigandset(&thread->sinfo.sigmask, &old, &kset);
	thread->sinfo.__update_pending();

	}
#endif

	hrtime_t timeout_ns = timespec_to_hrtime(&timeout);

	long res;
	if(utimeout)
		res = wait_for_event_timeout_interruptible(&wq, false, timeout_ns);
	else
	{
		res = sigtimedwait_forever(&wq);
	}

	if(res == -ETIMEDOUT)
		return -EAGAIN;

	spin_lock(&process->signal_lock);
	spin_lock(&thread->sinfo.lock);

	/* Find a pending signal */
	int signum = signal_find(thread);
	assert(signum > 0);

	/* If it's not a member of set, error out with EINTR(it will be handled on syscall return). */
	if(!sigismember(&kset, signum))
	{
		st = -EINTR;
		goto out;
	}

	/* As in the normal signal handling path, pop the sigpending */
	pending = signal_query_pending(signum,
                                  SIGNAL_QUERY_POP, &thread->sinfo);

	assert(pending != NULL);

	signal_unqueue(signum, thread);

	st = pending->signum;

	if(copy_to_user(info, pending->info, sizeof(siginfo_t)) < 0)
	{
		st = -EFAULT;
	}

	delete pending;

out:
	spin_unlock(&thread->sinfo.lock);
	spin_unlock(&process->signal_lock);

	return st;
}

int sys_rt_sigpending(sigset_t *uset, size_t sigsetlen)
{
	struct thread *current = get_current_thread();

	if(sigsetlen != CURRENT_SIGSETLEN)
		return -EINVAL;
	
	auto set = current->sinfo.get_pending_set();

	if(copy_to_user(uset, &set, sizeof(set)) < 0)
		return -EFAULT;
	
	return 0;
}

bool executing_in_altstack(const struct syscall_frame *frm, const stack_t *stack)
{
	/* TODO: This is arch-dependent and we'd probably be better off wrapping this access
	 * with a arch-dependent macro.
	 */
	/* TODO: This depends on whether the stack grows downwards or upwards. This logic covers the first case. */
	unsigned long sp = frm->user_sp;
	unsigned long alt_sp = (unsigned long) stack->ss_sp;
	unsigned long alt_stack_limit = alt_sp + stack->ss_size;
	return sp >= alt_sp && sp < alt_stack_limit;
}

static int alt_stack_sp_flags(const struct syscall_frame *frame, const stack_t *stack)
{
	return executing_in_altstack(frame, stack) ? SS_ONSTACK : 0;
}

#define VALID_SIGALTSTACK_FLAGS				(SS_AUTODISARM | SS_DISABLE)

int sys_sigaltstack(const stack_t *new_stack, stack_t *old_stack, const struct syscall_frame *frame)
{
	struct thread *current = get_current_thread();
	struct signal_info *sinfo = &current->sinfo;

	if(old_stack)
	{
		stack_t kold = {};
		kold.ss_sp = sinfo->altstack.ss_sp;
		kold.ss_size = sinfo->altstack.ss_size;
		kold.ss_flags = alt_stack_sp_flags(frame, &sinfo->altstack) | sinfo->altstack.ss_flags;

		if(copy_to_user(old_stack, &kold, sizeof(kold)) < 0)
			return -EFAULT;
	}

	if(new_stack)
	{
		stack_t stk;
		if(copy_from_user(&stk, new_stack, sizeof(stk)) < 0)
			return -EFAULT;
		
		if(stk.ss_flags & ~VALID_SIGALTSTACK_FLAGS)
			return -EINVAL;

		if(executing_in_altstack(frame, &sinfo->altstack))
			return -EPERM;
		
		stack_t *s = &sinfo->altstack;

		if(stk.ss_flags & SS_DISABLE)
		{
			/* We're disabling, zero out size and sp, and set the flag properly. */
			s->ss_flags = SS_DISABLE;
			s->ss_size = 0;
			s->ss_sp = NULL;
		}
		else
		{
			if(stk.ss_size < MINSIGSTKSZ)
				return -ENOMEM;

			s->ss_sp = stk.ss_sp;
			s->ss_size = stk.ss_size;
			s->ss_flags = stk.ss_flags;
		}
	}

	return 0;
}

void signal_info::reroute_signals(process *p)
{
	scoped_lock g{lock};

	list_for_every_safe(&pending_head)
	{
		auto pending = container_of(l, struct sigpending, list_node);

		list_remove(&pending->list_node);

		if(!p->route_signal(pending))
		{
			delete pending;
		}
	}
}

signal_info::~signal_info()
{
	scoped_lock g{lock};

	list_for_every_safe(&pending_head)
	{
		auto pending = container_of(l, struct sigpending, list_node);

		list_remove(&pending->list_node);
		delete pending;
	}
}
